{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMvYdYxxBgcq5rG9JDhh5VB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/DeepLearningFiles/Emotions_classification_tensorflow_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWPiTycZQGAa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the colab folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ENbsNywZkJAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Omdena Bhutan Chapter - Leveraging AI to Combat Mental Health Problems/Tasks/Task 2/combined_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Je3VbDcDkgtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "REhlOu7jk340"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"mental_state\"].value_counts()"
      ],
      "metadata": {
        "id": "MIpObi10k6xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "qdbgCKoJpqKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf = df.dropna()"
      ],
      "metadata": {
        "id": "dj-nhqMgptoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.info()"
      ],
      "metadata": {
        "id": "fEEdlfM0pwQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.isnull().sum()\n"
      ],
      "metadata": {
        "id": "ovWN-cZaqLPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: genereate the text data cleaning function for using nltk and regex\n",
        "# pre-process the text data using nltk and regex\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Clean the text data\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text"
      ],
      "metadata": {
        "id": "HQ4PG1uzpPZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf[\"cleaned_text\"] = ndf[\"text\"].apply(clean_text)\n",
        "ndf.head()"
      ],
      "metadata": {
        "id": "h56_YnNrppUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.sample(10)"
      ],
      "metadata": {
        "id": "8wfI39TZrutb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.drop(columns=\"text\", inplace=True)\n"
      ],
      "metadata": {
        "id": "xnN3c6yWrw0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf[\"mental_state\"].value_counts()"
      ],
      "metadata": {
        "id": "fpUTAkv7r989"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Classification by SMOTE train data balancing"
      ],
      "metadata": {
        "id": "rRAPVjeIJ0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's build keras model with the embeddings of the dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "text = ndf[\"cleaned_text\"].values\n",
        "labels = ndf[\"mental_state\"].values\n",
        "\n",
        "labels_encoded, labels_names = pd.factorize(labels)\n",
        "labels = to_categorical(labels_encoded, num_classes=len(labels_names))\n",
        "\n",
        "# tokenize the dataset\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "vocal_size = len(tokenizer.word_index) + 1\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")"
      ],
      "metadata": {
        "id": "OwLGfa-osDQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test split the model and SMOTE the input data\n",
        "from imblearn.keras import BalancedBatchGenerator\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# generate balanced data\n",
        "training_generator = BalancedBatchGenerator(X_train, y_train, batch_size=32, random_state=42)\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocal_size, 128, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(labels_names), activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# fit the model to the batch generator\n",
        "model.fit(training_generator, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "ikiYOBcIvf9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model is overfitting on the training dataset."
      ],
      "metadata": {
        "id": "iYFKCFZ6-6vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Keras classification using imbalance data without SMOTE"
      ],
      "metadata": {
        "id": "7RmExqkpLCjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# modeling using keras tf apis\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocal_size, 128, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(labels_names), activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# fit the model to the train dataset\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "DHnH4A9tLI8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Multi-label classification using imbalance data"
      ],
      "metadata": {
        "id": "VpstMyICLJcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset with 2 labels 1) emotion and 2) sentiment\n",
        "# developr the training of the tensorflow or keras model without data balance"
      ],
      "metadata": {
        "id": "vnSHksgmLPYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: Classifying text using bert-emotion using hugging face library"
      ],
      "metadata": {
        "id": "JBEnlE7Qmumf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "ndf.head()"
      ],
      "metadata": {
        "id": "LnydpLv_oNlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"boltuix/bert-emotion\"\n",
        "emotion_detection = pipeline(\"sentiment-analysis\", model=model_id)"
      ],
      "metadata": {
        "id": "hTHMUhxMm3Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = emotion_detection(ndf[\"cleaned_text\"][13454])\n",
        "output"
      ],
      "metadata": {
        "id": "M2W-qn8znan2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(ndf)\n",
        "dataset"
      ],
      "metadata": {
        "id": "slXxqSwaoctM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "tkXU57mRrdRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "def tokenizer_func(examples):\n",
        "  return tokenizer(examples[\"cleaned_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenizer_func, batched=True)"
      ],
      "metadata": {
        "id": "wAD8u1bvqBRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "NUPuJnL8rOxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the input data into pytorch format\n",
        "def pytorch_formate(example):\n",
        "  return {\n",
        "      \"input_ids\": torch.tensor(example[\"input_ids\"]),\n",
        "      \"attention_mask\": torch.tensor(example[\"attention_mask\"]),\n",
        "      \"labels\": example[\"labels\"]\n",
        "  }\n",
        "label_map = {name: i for i, name in enumerate(ndf[\"mental_state\"].unique())}\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(lambda examples: {\"labels\": [label_map[label] for label in examples[\"mental_state\"]]}, batched=True)\n",
        "\n",
        "#tokenized_data = tokenized_dataset.map(pytorch_formate)"
      ],
      "metadata": {
        "id": "wi9wXqgSrz_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training config and train the model\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "     output_dir=\"./bert_emotion_results\",\n",
        "     num_train_epochs=5,\n",
        "     per_device_train_batch_size=2,\n",
        "     logging_dir=\"./bert_emotion_logs\",\n",
        "     logging_steps=10,\n",
        "     save_steps=100,\n",
        "     eval_strategy=\"no\",\n",
        "     learning_rate=3e-5,\n",
        "     report_to=\"none\"  # Disable W&B auto-logging if not needed\n",
        " )\n",
        "\n",
        " # 6. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "     model=model,\n",
        "     args=training_args,\n",
        "     train_dataset=tokenized_dataset,\n",
        "     data_collator=data_collator,\n",
        " )\n",
        "\n",
        " # 7. Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "etqLcgIAtUS0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}