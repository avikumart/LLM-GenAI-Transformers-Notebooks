{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBsbZ1MYXv9teJc91gU6rH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/DeepLearningFiles/nn1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "givAU6VDy89w"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"nn.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "from numpy import exp\n",
        "\n",
        "class SimpleNetwork:\n",
        "    \"\"\"A simple feedforward network where all units have sigmoid activation.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def random(cls, *layer_units: int):\n",
        "        \"\"\"Creates a feedforward neural network with the given number of units\n",
        "        for each layer.\n",
        "\n",
        "        :param layer_units: Number of units for each layer\n",
        "        :return: the neural network\n",
        "        \"\"\"\n",
        "\n",
        "        def uniform(n_in, n_out):\n",
        "            epsilon = math.sqrt(6) / math.sqrt(n_in + n_out)\n",
        "            return np.random.uniform(-epsilon, +epsilon, size=(n_in, n_out))\n",
        "\n",
        "        pairs = zip(layer_units, layer_units[1:])\n",
        "        return cls(*[uniform(i, o) for i, o in pairs])\n",
        "\n",
        "    def __init__(self, *layer_weights: np.ndarray):\n",
        "        \"\"\"Creates a neural network from a list of weight matrices.\n",
        "        The weights correspond to transformations from one layer to the next, so\n",
        "        the number of layers is equal to one more than the number of weight\n",
        "        matrices.\n",
        "\n",
        "        :param layer_weights: A list of weight matrices\n",
        "        \"\"\"\n",
        "        self.weights = list(layer_weights)\n",
        "        self.num_layers = len(layer_weights) + 1\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            curr_weight = self.weights[i]\n",
        "            next_weight = self.weights[i + 1]\n",
        "            if curr_weight.shape[1] != next_weight.shape[0]:\n",
        "                raise ValueError(f\"incompatable dimentions between {i} and {i+1}\")\n",
        "\n",
        "    def forward_propagation(self, input_matrix: np.ndarray) -> List[np.ndarray]:\n",
        "        \"\"\"Performs forward propagation over the neural network\n",
        "              starting with\n",
        "           the given input matrix and returns all intermediate\n",
        "            activations\"\"\"\n",
        "         ##YOUR CODE HERE##\n",
        "        def sigmoid(x):\n",
        "          return 1 / (1 + exp(-x))\n",
        "        activations = [input_matrix]\n",
        "        current_activation = input_matrix\n",
        "\n",
        "        for i,w in enumerate(self.weights):\n",
        "          output = np.dot(current_activation, w)\n",
        "          current_activation = sigmoid(output)\n",
        "          activations.append(current_activation)\n",
        "\n",
        "        return activations\n",
        "\n",
        "    def predict(self, input_matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Performs forward propagation over the neural network starting with\n",
        "        the given input matrix.\n",
        "\n",
        "        Each unit's output should be calculated by taking a weighted sum of its\n",
        "        inputs (using the appropriate weight matrix) and passing the result of\n",
        "        that sum through a logistic sigmoid activation function.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :return: A matrix of predictions, where each row is the predicted\n",
        "        outputs - each in the range (0, 1) - for the corresponding row in the\n",
        "        input matrix.\n",
        "        \"\"\"\n",
        "        ##YOUR CODE HERE##\n",
        "        activations = self.forward_propagation(input_matrix)\n",
        "        return activations[-1]\n",
        "\n",
        "    def predict_zero_one(self, input_matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Performs forward propagation over the neural network starting with\n",
        "        the given input matrix, and converts the outputs to binary (0 or 1).\n",
        "\n",
        "        Outputs will be converted to 0 if they are less than 0.5, and converted\n",
        "        to 1 otherwise.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :return: A matrix of predictions, where each row is the predicted\n",
        "        outputs - each either 0 or 1 - for the corresponding row in the input\n",
        "        matrix.\n",
        "        \"\"\"\n",
        "        ##YOUR CODE HERE##\n",
        "        activations = self.forward_propagation(input_matrix)\n",
        "        return np.where(activations[-1] < 0.5, 0, 1)\n",
        "\n",
        "    def gradients(self,\n",
        "                  input_matrix: np.ndarray,\n",
        "                  output_matrix: np.ndarray) -> List[np.ndarray]:\n",
        "        \"\"\"Performs back-propagation to calculate the gradients for each of\n",
        "        the weight matrices.\n",
        "\n",
        "        This method first performs a pass of forward propagation through the\n",
        "        network, then applies the following procedure to calculate the\n",
        "        gradients. In the following description, × is matrix multiplication,\n",
        "        ⊙ is element-wise product, and ⊤ is matrix transpose.\n",
        "\n",
        "        First, calculate the error, error_L, between last layer's activations,\n",
        "        h_L, and the output matrix, y:\n",
        "\n",
        "        error_L = h_L - y\n",
        "\n",
        "        Then, for each layer l in the network, starting with the layer before\n",
        "        the output layer and working back to the first layer (the input matrix),\n",
        "        calculate the gradient for the corresponding weight matrix as follows.\n",
        "        First, calculate g_l as the element-wise product of the error for the\n",
        "        next layer, error_{l+1}, and the sigmoid gradient of the next layer's\n",
        "        weighted sum (before the activation function), a_{l+1}.\n",
        "\n",
        "        g_l = (error_{l+1} ⊙ sigmoid'(a_{l+1}))⊤\n",
        "\n",
        "        Then calculate the gradient matrix for layer l as the matrix\n",
        "        multiplication of g_l and the layer's activations, h_l, divided by the\n",
        "        number of input examples, N:\n",
        "\n",
        "        grad_l = (g_l × h_l)⊤ / N\n",
        "\n",
        "        Finally, calculate the error that should be backpropagated from layer l\n",
        "        as the matrix multiplication of the weight matrix for layer l and g_l:\n",
        "\n",
        "        error_l = (weights_l × g_l)⊤\n",
        "\n",
        "        Once this procedure is complete for all layers, the grad_l matrices\n",
        "        are the gradients that should be returned.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :param output_matrix: A matrix of expected outputs, where each row is\n",
        "        the expected outputs - each either 0 or 1 - for the corresponding row in\n",
        "        the input matrix.\n",
        "        :return: the gradient matrix for each weight matrix\n",
        "        \"\"\"\n",
        "        ##YOUR CODE HERE##\n",
        "        activations = self.forward_propagation(input_matrix) # get all the activations\n",
        "        h_l = self.predict(input_matrix)\n",
        "        error_l = h_l - output_matrix # error of the output layer\n",
        "\n",
        "        gradients = [None]*len(self.weights) # empty gradient matrix for each layer\n",
        "        next_layer_error = error_l\n",
        "\n",
        "        for l in range(len(self.weights) -1, -1, -1): # loop over the layer weights to calculate gradients\n",
        "          sig_derivative = activations[l+1] * (1 - activations[l+1])\n",
        "          g_l = np.multiply(next_layer_error, sig_derivative)\n",
        "          grad_l = np.dot(activations[l].T,g_l)\n",
        "          gradients[l] = grad_l / input_matrix.shape[0]\n",
        "          if l > 0:\n",
        "            next_layer_error = np.dot(g_l, self.weights[l].T)\n",
        "        return gradients\n",
        "\n",
        "\n",
        "    def train(self,\n",
        "              input_matrix: np.ndarray,\n",
        "              output_matrix: np.ndarray,\n",
        "              iterations: int = 10,\n",
        "              learning_rate: float = 0.1) -> None:\n",
        "        \"\"\"Trains the neural network on an input matrix and an expected output\n",
        "        matrix.\n",
        "\n",
        "        Training should repeatedly (`iterations` times) calculate the gradients,\n",
        "        and update the model by subtracting the learning rate times the\n",
        "        gradients from the model weight matrices.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :param output_matrix: A matrix of expected outputs, where each row is\n",
        "        the expected outputs - each either 0 or 1 - for the corresponding row in\n",
        "        the input matrix.\n",
        "        :param iterations: The number of gradient descent steps to take.\n",
        "        :param learning_rate: The size of gradient descent steps to take, a\n",
        "        number that the gradients should be multiplied by before updating the\n",
        "        model weights.\n",
        "        \"\"\"\n",
        "        ##YOUR CODE HERE##\n",
        "        for i in range(iterations):\n",
        "          gradients = self.gradients(input_matrix, output_matrix)\n",
        "          for l in range(len(self.weights)):\n",
        "            self.weights[l] -= learning_rate * gradients[l]"
      ]
    }
  ]
}