{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlPzHr9wi/cj9FEXOJXJPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/DeepLearningFiles/Pytorch_Hybrid_CLIP_T5_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWgeAUF6pFJ3"
      },
      "outputs": [],
      "source": [
        "# import the dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForCausalLM\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "import open_clip\n",
        "\n",
        "# from the preprocessing import labels\n",
        "MAX_TEXT_LEN = 256\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "\n",
        "# set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"avikalsingh/Clinical-T5-Base\", legacy=False)\n",
        "\n",
        "# add the vision wrapper to generate the embeddings\n",
        "class CLIPVisionWrapper(nn.Module):\n",
        "  def __init__(self, vision_model):\n",
        "    super().__init__()\n",
        "    self.vision_model = vision_model\n",
        "    self.embed_dim = getattr(vision_model, \"output_dim\", getattr(vision_model, \"embed_dim\", getattr(vision_model, \"width\", 512)))\n",
        "\n",
        "    token_dim = None\n",
        "    trunk = getattr(vision_model, \"trunk\",None)\n",
        "    if trunk is not None:\n",
        "      token_dim = getattr(trunk, \"embed_dim\", None)\n",
        "      if token_dim is None and hasattr(trunk, \"blocks\") and len(trunk.blocks) > 0:\n",
        "        token_dim = getattr(trunk.blocks[0], \"dim\", None)\n",
        "    self.token_dim = int(token_dmi) if token_dim is not None else int(self.embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    feats = self.vision(x)\n",
        "    if feats.ndim == 2: return feats\n",
        "    if feats.ndim == 4:\n",
        "      return 0.5 * (feats.mean(dim=(2,3)) + feats.amax(dim=(2,3)))\n",
        "    raise RuntimeError(\"Invalid input shape\")\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def forward_text(self, x):\n",
        "    v = self.vision\n",
        "    trunk = getattr(v, \"trunk\", None)\n",
        "    if trunk is not None and hasattr(trunk, \"forward_features\"):\n",
        "      out = trunk.forward_features(x)\n",
        "      if isinstance(out, torch.Tensor) and out.ndim == 3: return out\n",
        "    if hasattr(v, \"forward_features\"):\n",
        "      out = v.forward_features(x)\n",
        "      if isinstance(out, torch.Tensor) and out.ndim == 3: return out\n",
        "    pooled = self.forward(x)\n",
        "    return pooled.unsequeeze(1)\n",
        "\n",
        "\n",
        "# stage 1 for the classification\n",
        "def build_classifier(num_labels: int):\n",
        "  clip_model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\")\n",
        "  vision_encoder = CLIPVisionWrapper(clip_model.visual)\n",
        "  embed_dim = vision_encoder.embed_dim\n",
        "  classifier_head = nn.Linear(embed_dim, num_labels)\n",
        "  return nn.Sequential(vision_encoder, classifier_head)\n",
        "\n",
        "\n",
        "# multitask model for the hybrid training strategies\n",
        "class ClipMultiTaskModel(nn.Module):\n",
        "  def __init__(self, clip_encoder, text_decoder, num_labels: int, use_label_token: bool = True, label_token_scale: float = 0.5, max_visual_tokens: int = 64, use_decoder_aux: bool = False):\n",
        "    super().__init__()\n",
        "    self.clip_encoder = clip_encoder\n",
        "    self.text_decoder = text_decoder\n",
        "    self.num_labels = num_labels\n",
        "\n",
        "    # -- llama tokenizer for refinement ---\n",
        "    self.llama_tokeniner = AutoTokenizer.from_pretrained(\"meta-llam/Llama-3.3-1B\")\n",
        "    if self.llama_tokenizer.pad_token is None:\n",
        "      self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token\n",
        "\n",
        "    embed_dim = getattr(clip_encoder, \"embed_dim\", 512)\n",
        "    token_dim = getattr(clip_encoder, \"token_dim\", embed_dim)\n",
        "    d_model = text_decoder.config.d_model # T5 hidden size\n",
        "\n",
        "    self.enc_ln = nn.LayerNorm(d_model)\n",
        "    self.enc_gate = nn.Parameter(torch.tensor(1.0))\n",
        "    self.dec_ln = nn.LayerNorm(d_model)\n",
        "    self.class_head = nn.Linear(d_model, num_labels)\n",
        "\n",
        "    # ---- NEW NLP PROJECTOR ----\n",
        "    self.visual_projection = nn.Sequential(\n",
        "        nn.Linear(int(token_dim), int(d_model)),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(int(d_model), int(d_model)),\n",
        "        nn.Dropout(p=0.1)\n",
        "    )\n",
        "\n",
        "    self.max_visual_tokens = int(max_visual_tokens)\n",
        "    self.log_sigma_clf = nn.Parameter(torch.zeros(1))\n",
        "    self.log_sigma_gen = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    # llama 3 refiner\n",
        "    self.llm = AutoModelForCausalLM.from_pretrained(\"meta-llam/Llama-3.3-1B\")\n",
        "    for param in self.llm.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "    # auxiliary head (decoder side)\n",
        "    llama_dim = self.llm.config.hidden_size\n",
        "    self.use_decoder_aux = bool(use_decoder_aux)\n",
        "    self.decoder_cls_head = nn.Linear(llama_dim, num_labels) if self.use_decoder_aux else None\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      images,\n",
        "      clf_labels = None,\n",
        "      t5_labels =None,\n",
        "      decoder_attention_mask= None,\n",
        "      generate: bool = False,\n",
        "      generate_kwargs: dict | None = None,\n",
        "      **kwargs):\n",
        "\n",
        "     base_features = self.clip_encoder(images)\n",
        "     vis_tokens = self.clip_encoder.forward_tokens(images)\n",
        "\n",
        "     if vis_tokens.size(1) > self.max_visual_tokens:\n",
        "      idx = torch.linspace(0, vis_tokens.size(1) -1, steps=self.max_visual_tokens).long().to(vis_tokens.device)\n",
        "      vis_tokens = vis_tokens[:, idx, :]\n",
        "\n",
        "\n",
        "     encoder_hidden = self.visual_projection(vis_tokens)\n",
        "     encoder_hidden = self.enc_ln(encoder_hidden)\n",
        "     gate = torch.sigmoid(self.enc_gate)\n",
        "     encoder_hidden = gate * encoder_hidden\n",
        "\n",
        "\n",
        "     # create the attention mask for te T5\n",
        "     enc_attn_mask = torch.ones((images.size(0), encoder_hidden.size(1)), dtype=torch.long, device=images.device)\n",
        "     encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden)\n",
        "\n",
        "     # stage 1 clfs\n",
        "     class_logits = self.class_head(base_features)\n",
        "\n",
        "     # generation mode:\n",
        "     if generate:\n",
        "      if generate_kwargs is None: generate_kwargs = {}\n",
        "            # Defaults for clinical accuracy\n",
        "      generate_kwargs.setdefault(\"max_new_tokens\", 128)\n",
        "      generate_kwargs.setdefault(\"num_beams\", 4)\n",
        "      generate_kwargs.setdefault(\"do_sample\", False)\n",
        "      generate_kwargs.setdefault(\"decoder_start_token_id\", self.decoder.config.pad_token_id or 0)\n",
        "\n",
        "            # Step A: Generate draft with T5 (using projected visual tokens)\n",
        "      t5_gen_ids = self.decoder.generate(\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                attention_mask=enc_attn_mask,\n",
        "                **generate_kwargs,\n",
        "            )\n",
        "\n",
        "            # Step B: Decode T5 output to text\n",
        "      draft_texts = tokenizer.batch_decode(t5_gen_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Step C: Refine with Llama\n",
        "            # Prompt Engineering for Refinement\n",
        "      refinement_prompts = [\n",
        "                f\"Fix grammar and medical terminology in this report:\\nDraft: {txt}\\nRefined Report:\"\n",
        "                for txt in draft_texts\n",
        "            ]\n",
        "\n",
        "      llama_inputs = self.llama_tokenizer(\n",
        "                refinement_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            ).to(images.device)\n",
        "\n",
        "      final_ids = self.llm.generate(\n",
        "                **llama_inputs,\n",
        "                max_new_tokens=128,\n",
        "                do_sample=True, # Slight sampling for natural flow in refinement\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "      return class_logits, final_ids\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # TRAINING MODE\n",
        "        # ---------------------------------------------------------\n",
        "     if t5_labels is None:\n",
        "      return class_logits, None\n",
        "\n",
        "        # T5 Forward\n",
        "     outputs = self.decoder(\n",
        "            labels=t5_labels,\n",
        "            attention_mask=enc_attn_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask = decoder_attention_mask,\n",
        "            output_hidden_states=self.use_decoder_aux,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "      # Llama Aux Head Logic\n",
        "     aux_logits = None\n",
        "     if self.use_decoder_aux:\n",
        "            with torch.no_grad():\n",
        "                # Re-tokenize T5 labels for Llama\n",
        "                t5_labels_replaced = t5_labels.clone()\n",
        "                t5_labels_replaced[t5_labels_replaced == -100] = tokenizer.pad_token_id\n",
        "                decoded_labels = tokenizer.batch_decode(t5_labels_replaced, skip_special_tokens=True)\n",
        "\n",
        "                llama_inputs = self.llama_tokenizer(\n",
        "                    decoded_labels,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True\n",
        "                ).to(images.device)\n",
        "\n",
        "                llm_outputs = self.llm(\n",
        "                    input_ids=llama_inputs.input_ids,\n",
        "                    attention_mask=llama_inputs.attention_mask,\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "\n",
        "            # Feature Extraction (Last Hidden State Mean Pool)\n",
        "            hidden_states = llm_outputs.hidden_states[-1]\n",
        "            decoder_hidden_pooled = hidden_states.mean(dim=1)\n",
        "            aux_logits = self.decoder_cls_head(decoder_hidden_pooled)\n",
        "\n",
        "     return class_logits, outputs, aux_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BK-qdeHn_H2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}