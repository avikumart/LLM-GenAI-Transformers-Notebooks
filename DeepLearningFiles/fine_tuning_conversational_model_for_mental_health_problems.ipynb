{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMCCSTAWzkQwhgvtsh82D9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/DeepLearningFiles/fine_tuning_conversational_model_for_mental_health_problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow -q"
      ],
      "metadata": {
        "id": "6mKTdPCnoaJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSiF6X4Omfug"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EtOUObE0oDRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Omdena Bhutan Chapter - Leveraging AI to Combat Mental Health Problems/Tasks/Task 2/combined_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "J5SIusmEoXbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "J0AcgkBkpve-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf = df.dropna()"
      ],
      "metadata": {
        "id": "QISar_0cqhEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.info()"
      ],
      "metadata": {
        "id": "SYjJZv2-qjvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Clean the text data\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text"
      ],
      "metadata": {
        "id": "4fWzHOrjpwUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the cleaned text\n",
        "ndf[\"cleaned_text\"] = ndf[\"text\"].apply(clean_text)\n",
        "ndf.head()"
      ],
      "metadata": {
        "id": "nPixzEZWqJpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.drop(columns=\"text\", inplace=True)"
      ],
      "metadata": {
        "id": "YLP7rjECqwhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.head()"
      ],
      "metadata": {
        "id": "gwgKoubcrhjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf[\"mental_state\"].value_counts()"
      ],
      "metadata": {
        "id": "tybkEKiBtltO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the function to covert the normal state as 1 and rest of the normal state as 0\n",
        "def sentiment(df,column):\n",
        "  df[column] = df[column].apply(lambda x: 1 if x == \"normal\" else 0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "W0I7fZM6tvRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf[\"sentiment\"] = ndf[\"mental_state\"].apply(lambda x: 1 if x == \"normal\" else 0)\n",
        "ndf[ndf[\"sentiment\"] == 1].head()"
      ],
      "metadata": {
        "id": "Esy6-Qa3uHgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndf[\"mental_state\"].nunique()"
      ],
      "metadata": {
        "id": "cEWAzu0OzrhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development: Multiple output model for the sentiment and emotion classification"
      ],
      "metadata": {
        "id": "FtAfCuFJqzjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's build keras model with the embeddings of the dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "text = ndf[\"cleaned_text\"].values\n",
        "labels = ndf[\"mental_state\"].values\n",
        "label2 = ndf[\"sentiment\"].values\n",
        "\n",
        "labels_encoded, labels_names = pd.factorize(labels)\n",
        "labels = to_categorical(labels_encoded, num_classes=len(labels_names))\n",
        "\n",
        "# tokenize the dataset\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "vocal_size = len(tokenizer.word_index) + 1\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")"
      ],
      "metadata": {
        "id": "aueCF7GHq6nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "metadata": {
        "id": "jDEU7E63wECq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model architecture for two prediction outputs\n",
        "from keras.layers import Dense, Dropout, GRU, BatchNormalization, Input, GlobalMaxPooling1D, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout"
      ],
      "metadata": {
        "id": "kYDUYLT-s-v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multioutput_model(vocab_size, max_length, embedding_dim, num_emotions, num_sentiment):\n",
        "  \"\"\"\n",
        "  model architecture code for the multioutput model\n",
        "\n",
        "  Args:\n",
        "  vocab_size: total number of words in vocabulary\n",
        "  max_length: maximul lenght of sequence\n",
        "  embedding_dim: dimension of embedding layer\n",
        "  num_emotions: total number of emotions\n",
        "  num_sentiment: total number of sentiments\n",
        "\n",
        "  Returns:\n",
        "  model: multioutput model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  input_text =  Input(shape=(max_length,))\n",
        "  embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_text)\n",
        "  # lstm for the emotions detection and sentiment detection\n",
        "  lstm1 = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "  lstm1 = Dropout(0.2)(lstm1)\n",
        "  lstm2 = LSTM(64)(lstm1)\n",
        "  lstm2 = Dropout(0.2)(lstm2)\n",
        "  pooled_features = GlobalMaxPooling1D()(embedding_layer)\n",
        "  # combine the features\n",
        "  combined_features = Concatenate()([lstm2, pooled_features])\n",
        "  # two dense layers\n",
        "  dense1 = Dense(128, activation='relu')(combined_features)\n",
        "  dense1 = BatchNormalization()(dense1)\n",
        "  dense1 = Dropout(0.2)(dense1)\n",
        "  dense2 = Dense(64, activation='relu')(dense1)\n",
        "  dense2 = BatchNormalization()(dense2)\n",
        "  dense2 = Dropout(0.2)(dense2)\n",
        "  # emotion detection output dense layers\n",
        "  emotion_dense1 = Dense(64, activation='relu')(dense2)\n",
        "  emotion_dense1 = BatchNormalization()(emotion_dense1)\n",
        "  emotion_dense1 = Dropout(0.2)(emotion_dense1)\n",
        "  emotion_output = Dense(num_emotions, activation='softmax', name='emotion_output')(emotion_dense1)\n",
        "  # sentiment detection output layers\n",
        "  senti_dense1 = Dense(64, activation='relu')(dense2)\n",
        "  senti_dense1 = BatchNormalization()(senti_dense1)\n",
        "  senti_dense1 = Dropout(0.2)(senti_dense1)\n",
        "  senti_output = Dense(num_sentiment, activation='sigmoid', name='senti_output')(senti_dense1)\n",
        "\n",
        "  model = Model(inputs=input_text, outputs=[emotion_output, senti_output])\n",
        "  return model"
      ],
      "metadata": {
        "id": "Y1zM_nL8vwZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import keras metrics\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseCategoricalCrossentropy, BinaryAccuracy, BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras"
      ],
      "metadata": {
        "id": "EUpqcHFe4lpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "def compile_model(model, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Compile the multi-output model with appropriate losses and metrics.\n",
        "\n",
        "    Args:\n",
        "        model: Keras model to compile\n",
        "        learning_rate: Learning rate for optimizer\n",
        "\n",
        "    Returns:\n",
        "        Compiled model\n",
        "    \"\"\"\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss={\n",
        "            'emotion_output': SparseCategoricalCrossentropy(),\n",
        "            'senti_output': BinaryCrossentropy()\n",
        "        },\n",
        "        loss_weights={\n",
        "            'emotion_output': 1.0,\n",
        "            'senti_output': 1.0\n",
        "        },\n",
        "        metrics={\n",
        "            'emotion_output': [SparseCategoricalAccuracy(name='emotion_accuracy')],\n",
        "            'senti_output': [BinaryAccuracy(name='sentiment_accuracy')]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6mao3LSK3s3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model = multioutput_model(vocal_size, max_length, 100, 9, 2)\n",
        "model = compile_model(model)"
      ],
      "metadata": {
        "id": "PwdeToJ85Ti-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "9OQZJDI46EY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "label2_one_hot = to_categorical(label2, num_classes=2)\n",
        "\n",
        "history = model.fit(\n",
        "        padded_sequences,\n",
        "        {\n",
        "            'emotion_output': labels,\n",
        "            'senti_output': label2_one_hot\n",
        "        },\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        callbacks=[\n",
        "            keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "            keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5)\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "B9kw4QA06Ft5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}