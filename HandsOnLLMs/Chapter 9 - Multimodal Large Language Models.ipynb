{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/HandsOnLLMs/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>Chapter 9 - Multimodal Large Language Models</h1>\n",
        "<i>Adding vision capabilities to Large Language Models.</i>\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
        "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
        "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter09/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is for Chapter 9 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
        "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD5eRiQe4ooH"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hir0okF4ooI"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install matplotlib transformers datasets accelerate sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMF9p8qK58Ou"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHUY1IBzZkgy"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "# Load an AI-generated image of a puppy playing in the snow\n",
        "puppy_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png\"\n",
        "image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
        "caption = \"a puppy playing in the snow\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHOmvsRFZp4B"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpAwAij0at4o"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLaZrGSxavbk"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
        "\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Load a tokenizer to preprocess the text\n",
        "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
        "\n",
        "# Load a processor to preprocess the images\n",
        "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Main model for generating text and image embeddings\n",
        "model = CLIPModel.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSx-X0-ecBv_"
      },
      "outputs": [],
      "source": [
        "# Tokenize our input\n",
        "inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVyHP2YwdUqB"
      },
      "outputs": [],
      "source": [
        "# Convert our input back to tokens\n",
        "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEj4b22qeCm8"
      },
      "outputs": [],
      "source": [
        "# Create a text embedding\n",
        "text_embedding = model.get_text_features(**inputs)\n",
        "text_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding"
      ],
      "metadata": {
        "id": "oiu7Xt5K_XIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwNQpueperGn"
      },
      "outputs": [],
      "source": [
        "# Preprocess image\n",
        "processed_image = clip_processor(\n",
        "    text=None, images=image, return_tensors='pt'\n",
        ")['pixel_values']\n",
        "\n",
        "processed_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_image"
      ],
      "metadata": {
        "id": "to7U3uZnBwKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P9I7JsGf7Km"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare image for visualization\n",
        "img = processed_image.squeeze(0)\n",
        "img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
        "img = np.einsum('ijk->jik', img)\n",
        "\n",
        "# Visualize preprocessed image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVFva16chlS3"
      },
      "outputs": [],
      "source": [
        "# Create the image embedding\n",
        "image_embedding = model.get_image_features(processed_image)\n",
        "image_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IwyPRa0R964"
      },
      "outputs": [],
      "source": [
        "# Normalize the embeddings\n",
        "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
        "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Calculate their similarity\n",
        "text_embedding = text_embedding.detach().cpu().numpy()\n",
        "image_embedding = image_embedding.detach().cpu().numpy()\n",
        "score = text_embedding @ image_embedding.T\n",
        "score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rKItoQAkQwU"
      },
      "source": [
        "### More Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMsrjH7xkSWH"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "# Load an AI-generated image of a puppy playing in the snow\n",
        "cat_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/cat.png\"\n",
        "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
        "paths = [puppy_path, cat_path, car_path]\n",
        "images = [Image.open(urlopen(path)).convert(\"RGBA\") for path in paths]\n",
        "captions = [\n",
        "    \"a puppy playing in the snow\",\n",
        "    \"a pixelated image of a cute cat\",\n",
        "    \"A supercar on the road \\nwith the sunset in the background\"\n",
        "]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Embed all images\n",
        "image_embeddings = []\n",
        "for image in images:\n",
        "  image_processed = clip_processor(images=image, return_tensors='pt')['pixel_values']\n",
        "  image_embedding = model.get_image_features(image_processed).detach().cpu().numpy()[0]\n",
        "  image_embeddings.append(image_embedding)\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "\n",
        "# Embed all captions\n",
        "text_embeddings = []\n",
        "for caption in captions:\n",
        "  inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
        "  text_emb = model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
        "  text_embeddings.append(text_emb)\n",
        "text_embeddings = np.array(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ABSnFGlGeW"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between images and captions\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sim_matrix = cosine_similarity(image_embeddings, text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmIyzyTXlIRN"
      },
      "outputs": [],
      "source": [
        "# Create base figure\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(sim_matrix, cmap='viridis')\n",
        "\n",
        "# Adjust ticks with correct labels\n",
        "plt.yticks(range(len(captions)), captions, fontsize=18)\n",
        "plt.xticks([])\n",
        "\n",
        "# Visualize\n",
        "for i, image in enumerate(images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "\n",
        "# Add the captions at the correct indices\n",
        "for x in range(sim_matrix.shape[1]):\n",
        "    for y in range(sim_matrix.shape[0]):\n",
        "        plt.text(x, y, f\"{sim_matrix[y, x]:.2f}\", ha=\"center\", va=\"center\", size=30)\n",
        "\n",
        "# Remove unnecessary spines\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "# Resize blocks\n",
        "plt.xlim([-0.5, len(captions) - 0.5])\n",
        "plt.ylim([len(captions) + 0.5, -2])\n",
        "# plt.title(\"Similarity Matrix\", size=20)\n",
        "plt.savefig(\"sim_matrix.png\", dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5it3Vqi0Bf5"
      },
      "source": [
        "### SBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XFEvSss0DQf"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load SBERT-compatible CLIP model\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# Encode the images\n",
        "image_embeddings = model.encode(images)\n",
        "\n",
        "# Encode the captions\n",
        "text_embeddings = model.encode(captions)\n",
        "\n",
        "#Compute cosine similarities\n",
        "sim_matrix = util.cos_sim(image_embeddings, text_embeddings)\n",
        "print(sim_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsLAEG2a5-5r"
      },
      "source": [
        "## BLIP-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnNrafWu6BSJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load processor and main model\n",
        "blip_processor = AutoProcessor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    revision=\"51572668da0eb669e01a189dc22abe6088589a24\"  # Choose specific model because of: https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/39\n",
        ")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    revision=\"51572668da0eb669e01a189dc22abe6088589a24\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Send the model to GPU to speed up inference\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI_UWlR8c_Ey"
      },
      "source": [
        "### Preprocessing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6YeV_TEQFAA"
      },
      "outputs": [],
      "source": [
        "# Load image of a supercar\n",
        "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flZeiDFmQIIg"
      },
      "outputs": [],
      "source": [
        "# Preprocess the image\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "inputs[\"pixel_values\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lHJ7LiKUhWf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Convert to numpy and go from (1, 3, 224, 224) to (224, 224, 3) in shape\n",
        "image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
        "image_inputs = np.einsum('ijk->kji', image_inputs)\n",
        "image_inputs = np.einsum('ijk->jik', image_inputs)\n",
        "\n",
        "# Scale image inputs to 0-255 to represent RGB values\n",
        "scaler = MinMaxScaler(feature_range=(0, 255))\n",
        "image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
        "image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
        "\n",
        "# Convert numpy array to Image\n",
        "Image.fromarray(image_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDoBrN56dBTF"
      },
      "source": [
        "### Preprocessing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHJkauW9dFhB"
      },
      "outputs": [],
      "source": [
        "blip_processor.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Zh5Rx8QLAy"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text\n",
        "text = \"Her vocalization was remarkably melodic\"\n",
        "token_ids = blip_processor(image, text=text, return_tensors=\"pt\")\n",
        "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
        "\n",
        "# Convert input ids back to tokens\n",
        "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFnL_Yt3elPX"
      },
      "outputs": [],
      "source": [
        "# Replace the space token with an underscore\n",
        "tokens = [token.replace(\"Ä \", \"_\") for token in tokens]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDx5elnHeHnT"
      },
      "source": [
        "### Use Case 1: Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJ9-l8c8u3i"
      },
      "outputs": [],
      "source": [
        "# Load an AI-generated image of a supercar\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
        "\n",
        "# Convert an image into inputs and preprocess it\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHIoTZa6eEaR"
      },
      "outputs": [],
      "source": [
        "# Generate image ids to be passed to the decoder (LLM)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "# Generate text from the image ids\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbGnyiYhlfQi"
      },
      "outputs": [],
      "source": [
        "url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\n",
        "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bIxPEySrJW0"
      },
      "outputs": [],
      "source": [
        "# Load rorschach image\n",
        "url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_blot_01.jpg\"\n",
        "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
        "\n",
        "# Generate caption\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjHfQxWGkVYF"
      },
      "source": [
        "### Use Case 2: Visual Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "269NtauNFIze"
      },
      "outputs": [],
      "source": [
        "# Load an AI-generated image of a supercar\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NsMF_hMCIIj"
      },
      "outputs": [],
      "source": [
        "# Visual Question Answering\n",
        "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
        "\n",
        "# Process both the image and the prompt\n",
        "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# Generate text\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtpMIgUaScBD"
      },
      "outputs": [],
      "source": [
        "# Chat-like prompting\n",
        "prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"\n",
        "\n",
        "# Generate output\n",
        "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtvYbcF-H_t6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def text_eventhandler(*args):\n",
        "  question = args[0][\"new\"]\n",
        "  if question:\n",
        "    args[0][\"owner\"].value = \"\"\n",
        "\n",
        "    # Create prompt\n",
        "    if not memory:\n",
        "      prompt = \" Question: \" + question + \" Answer:\"\n",
        "    else:\n",
        "      template = \"Question: {} Answer: {}.\"\n",
        "      prompt = \" \".join(\n",
        "          [\n",
        "              template.format(memory[i][0], memory[i][1])\n",
        "              for i in range(len(memory))\n",
        "          ]\n",
        "      ) + \" Question: \" + question + \" Answer:\"\n",
        "\n",
        "    # Generate text\n",
        "    inputs = blip_processor(image, text=prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device, torch.float16)\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
        "    generated_text = blip_processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    generated_text = generated_text[0].strip().split(\"Question\")[0]\n",
        "\n",
        "    # Update memory\n",
        "    memory.append((question, generated_text))\n",
        "\n",
        "    # Assign to output\n",
        "    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n",
        "    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n",
        "    output.append_display_data(HTML(\"<br>\"))\n",
        "\n",
        "# Prepare widgets\n",
        "in_text = widgets.Text()\n",
        "in_text.continuous_update = False\n",
        "in_text.observe(text_eventhandler, \"value\")\n",
        "output = widgets.Output()\n",
        "memory = []\n",
        "\n",
        "# Display chat box\n",
        "display(\n",
        "    widgets.VBox(\n",
        "        children=[output, in_text],\n",
        "        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}