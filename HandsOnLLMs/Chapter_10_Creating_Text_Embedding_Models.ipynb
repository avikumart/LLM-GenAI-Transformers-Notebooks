{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/HandsOnLLMs/Chapter_10_Creating_Text_Embedding_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>Chapter 10 - Creating Text Embedding Models</h1>\n",
        "<i>Exploring methods for both training and fine-tuning embedding models.</i>\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
        "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
        "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter10/Chapter%2010%20-%20Creating%20Text%20Embedding%20Models.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is for Chapter 10 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
        "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8wVYr8edJAU"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        "üí° **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vu-Y3d-dJAW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q accelerate>=0.27.2 peft>=0.9.0 bitsandbytes>=0.43.0 transformers>=4.38.2 trl>=0.7.11 sentencepiece>=0.1.99\n",
        "!pip install -q sentence-transformers>=3.0.0 mteb>=1.1.2 datasets>=2.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UrKluX5YNmu"
      },
      "source": [
        "# Creating an Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywsyZzm5VSER"
      },
      "source": [
        "## **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahk0SJDKVy6F"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-BHO4-qwMDO"
      },
      "outputs": [],
      "source": [
        "train_dataset[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wO23cXLXeFU"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4qLaPR6nrqC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Use a base model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAiL21AuYKVI"
      },
      "source": [
        "## **Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgmtKckBXiK9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "# Define the loss function. In soft-max loss, we will also need to explicitly set the number of labels.\n",
        "train_loss = losses.SoftmaxLoss(\n",
        "    model=embedding_model,\n",
        "    sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimension(),\n",
        "    num_labels=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0efspwlOX2"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ZsoY0AretV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umikSmoYIP07"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uAAhNs0ocoV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"base_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKA_L39FpAoM"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "\n",
        "# Train embedding model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NA16lEaseOq"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xjkvCWwrp_"
      },
      "source": [
        "# MTEB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hueu4upxVYb"
      },
      "outputs": [],
      "source": [
        "from mteb import MTEB\n",
        "\n",
        "# Choose evaluation task\n",
        "evaluation = MTEB(tasks=[\"Banking77Classification\"])\n",
        "\n",
        "# Calculate results\n",
        "results = evaluation.run(embedding_model)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56V2ma89uJwN"
      },
      "source": [
        "‚ö†Ô∏è **VRAM Clean-up** - You will need to run the code below to partially empty the VRAM (GPU RAM). If that does not work, it is advised to restart the notebook instead. You can check the resources on the right-hand side (if you are using Google Colab) to check whether the used VRAM is indeed low. You can also run `!nivia-smi` to check current usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3LX1G0_4QCv"
      },
      "outputs": [],
      "source": [
        "# # Empty and delete trainer/model\n",
        "# trainer.accelerator.clear()\n",
        "# del trainer, embedding_model\n",
        "\n",
        "# # Garbage collection and empty cache\n",
        "# import gc\n",
        "# import torch\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d0GcY8cnNs4"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnRRSDN06eB"
      },
      "source": [
        "# Loss Fuctions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSCWbFO7RRM"
      },
      "source": [
        "‚ö†Ô∏è **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq8Yb6IB2LFI"
      },
      "source": [
        "## Cosine Similarity Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEmnjQQPuszQ"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "# (neutral/contradiction)=0 and (entailment)=1\n",
        "mapping = {2: 0, 1: 0, 0:1}\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"sentence1\": train_dataset[\"premise\"],\n",
        "    \"sentence2\": train_dataset[\"hypothesis\"],\n",
        "    \"label\": [float(mapping[label]) for label in train_dataset[\"label\"]]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np5bMwgO5y8g"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikky866vdseY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"cosineloss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E69gBMG46WVF"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnGpHqy46YS3"
      },
      "source": [
        "‚ö†Ô∏è **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvps7UpznPD4"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yh0toLx2Ni7"
      },
      "source": [
        "## Multiple Negatives Ranking Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzToWFH0vZzz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# # Load MNLI dataset from GLUE\n",
        "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "mnli = mnli.remove_columns(\"idx\")\n",
        "mnli = mnli.filter(lambda x: True if x['label'] == 0 else False)\n",
        "\n",
        "# Prepare data and add a soft negative\n",
        "train_dataset = {\"anchor\": [], \"positive\": [], \"negative\": []}\n",
        "soft_negatives = mnli[\"hypothesis\"]\n",
        "random.shuffle(soft_negatives)\n",
        "for row, soft_negative in tqdm(zip(mnli, soft_negatives)):\n",
        "    train_dataset[\"anchor\"].append(row[\"premise\"])\n",
        "    train_dataset[\"positive\"].append(row[\"hypothesis\"])\n",
        "    train_dataset[\"negative\"].append(soft_negative)\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP_s1yAB7D7I"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Q2m0yzkRvW"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"mnrloss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvPEvgf98uS8"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND1ej1ag054E"
      },
      "source": [
        "# **Fine-tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqATI-1V7coM"
      },
      "source": [
        "‚ö†Ô∏è **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkWj0SfYnRFd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGxyXucEkjfw"
      },
      "source": [
        "## **Supervised**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GXBTm_C-IPE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ3vW2VTA9dN"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"finetuned_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaPJIpkS-ZrT"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pHpVCwmk-XW"
      },
      "outputs": [],
      "source": [
        "# Evaluate the pre-trained model\n",
        "original_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "evaluator(original_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii6sIMpH7d7S"
      },
      "source": [
        "‚ö†Ô∏è **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGCTfC-unSL1"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvCPXCSZkkxm"
      },
      "source": [
        "## **Augmented SBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtoEArJElrZh"
      },
      "source": [
        "**Step 1:** Fine-tune a cross-encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJhEDAeeyhPD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from sentence_transformers import InputExample\n",
        "from sentence_transformers.datasets import NoDuplicatesDataLoader\n",
        "\n",
        "# Prepare a small set of 10000 documents for the cross-encoder\n",
        "dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000))\n",
        "mapping = {2: 0, 1: 0, 0:1}\n",
        "\n",
        "# Data Loader\n",
        "gold_examples = [\n",
        "    InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=mapping[row[\"label\"]])\n",
        "    for row in tqdm(dataset)\n",
        "]\n",
        "gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size=32)\n",
        "\n",
        "# Pandas DataFrame for easier data handling\n",
        "gold = pd.DataFrame(\n",
        "    {\n",
        "    'sentence1': dataset['premise'],\n",
        "    'sentence2': dataset['hypothesis'],\n",
        "    'label': [mapping[label] for label in dataset['label']]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_MHAJzl2H6Z"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "# Train a cross-encoder on the gold dataset\n",
        "cross_encoder = CrossEncoder('bert-base-uncased', num_labels=2)\n",
        "cross_encoder.fit(\n",
        "    train_dataloader=gold_dataloader,\n",
        "    epochs=1,\n",
        "    show_progress_bar=True,\n",
        "    warmup_steps=100,\n",
        "    use_amp=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0OcVG6WmMOJ"
      },
      "source": [
        "**Step 2:** Create new sentence pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgx8N8a8kVrZ"
      },
      "outputs": [],
      "source": [
        "# Prepare the silver dataset by predicting labels with the cross-encoder\n",
        "silver = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000, 50_000))\n",
        "pairs = list(zip(silver['premise'], silver['hypothesis']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcG7cDG5qrwX"
      },
      "source": [
        "**Step 3:** Label new sentence pairs with the fine-tuned cross-encoder (silver dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9Yuhzxq2NMj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Label the sentence pairs using our fine-tuned cross-encoder\n",
        "output = cross_encoder.predict(pairs, apply_softmax=True, show_progress_bar=True)\n",
        "silver = pd.DataFrame(\n",
        "    {\n",
        "        \"sentence1\": silver[\"premise\"],\n",
        "        \"sentence2\": silver[\"hypothesis\"],\n",
        "        \"label\": np.argmax(output, axis=1)\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Jd-Kssqzk_"
      },
      "source": [
        "**Step 4:** Train a bi-encoder (SBERT) on the extended dataset (gold + silver dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp09qxMhzagi"
      },
      "outputs": [],
      "source": [
        "# Combine gold + silver\n",
        "data = pd.concat([gold, silver], ignore_index=True, axis=0)\n",
        "data = data.drop_duplicates(subset=['sentence1', 'sentence2'], keep=\"first\")\n",
        "train_dataset = Dataset.from_pandas(data, preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-6RW_wOAOwO"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK1KybOI_uIY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"augmented_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_NHjK75z58G"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwAaFBHvDcFi"
      },
      "outputs": [],
      "source": [
        "trainer.accelerator.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX6lArIH0h1A"
      },
      "source": [
        "**Step 5**: Evaluate without silver dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyPBGfxp0D_7"
      },
      "outputs": [],
      "source": [
        "# Combine gold + silver\n",
        "data = pd.concat([gold], ignore_index=True, axis=0)\n",
        "data = data.drop_duplicates(subset=['sentence1', 'sentence2'], keep=\"first\")\n",
        "train_dataset = Dataset.from_pandas(data, preserve_index=False)\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"gold_only_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L8_5TLJ0jdK"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZujSUsu0sHU"
      },
      "source": [
        "Compared to using both the silver and gold datasets, using only the gold dataset reduces the performance of the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVq3FSZL7gK7"
      },
      "source": [
        "‚ö†Ô∏è **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gckDRJ1nUfo"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7RNAKVl3wmM"
      },
      "source": [
        "## **Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq_phjTb31gX"
      },
      "source": [
        "### Tranformer-based Denoising AutoEncoder (TSDAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yMdUf_WwErS"
      },
      "outputs": [],
      "source": [
        "# Download additional tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruI-lOZYZt7J"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datasets import Dataset, load_dataset\n",
        "from sentence_transformers.datasets import DenoisingAutoEncoderDataset\n",
        "\n",
        "# Create a flat list of sentences\n",
        "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(25_000))\n",
        "flat_sentences = mnli[\"premise\"] + mnli[\"hypothesis\"]\n",
        "\n",
        "# Add noise to our input data\n",
        "damaged_data = DenoisingAutoEncoderDataset(list(set(flat_sentences)))\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = {\"damaged_sentence\": [], \"original_sentence\": []}\n",
        "for data in tqdm(damaged_data):\n",
        "    train_dataset[\"damaged_sentence\"].append(data.texts[0])\n",
        "    train_dataset[\"original_sentence\"].append(data.texts[1])\n",
        "train_dataset = Dataset.from_dict(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mymxiQ9A1eQm"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77IuQ8QIjO25"
      },
      "outputs": [],
      "source": [
        "# # Choose a different deletion ratio\n",
        "# flat_sentences = list(set(flat_sentences))\n",
        "# damaged_data = DenoisingAutoEncoderDataset(\n",
        "#     flat_sentences,\n",
        "#     noise_fn=lambda s: DenoisingAutoEncoderDataset.delete(s, del_ratio=0.6)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl6CzdwNA1tC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYM298tWlacT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "# Create your embedding model\n",
        "word_embedding_model = models.Transformer('bert-base-uncased')\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
        "embedding_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RZ8tQFSlIHm"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "# Use the denoising auto-encoder loss\n",
        "train_loss = losses.DenoisingAutoEncoderLoss(\n",
        "    embedding_model, tie_encoder_decoder=True\n",
        ")\n",
        "train_loss.decoder = train_loss.decoder.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYApurOS07x0"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"tsdae_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGxh6fTa7qIh"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAd7OKerm-w8"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}