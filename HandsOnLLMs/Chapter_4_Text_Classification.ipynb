{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/HandsOnLLMs/Chapter_4_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_a9QvUFVCUR"
      },
      "source": [
        "<h1>Chapter 4 - Text Classification</h1>\n",
        "<i>Classifying text with both representative and generative models</i>\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
        "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
        "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is for Chapter 4 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
        "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp18T3Li7lVY"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxZV0-DQ7lVZ"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install datasets transformers sentence-transformers openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeVnXxQWy7-"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5phRS_z2U_3T"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our data\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJJmaJzHDLZv"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya5dfmVoR1R"
      },
      "source": [
        "# **Text Classification with Representation Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co68g-Eloknf"
      },
      "source": [
        "## **Using a Task-specific Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph-3T3XJopdN"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Path to our HF model\n",
        "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# Load model into pipeline\n",
        "pipe = pipeline(\n",
        "    model=model_path,\n",
        "    tokenizer=model_path,\n",
        "    return_all_scores=True,\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2gbnL5Q69Y5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "# Run inference\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n",
        "    negative_score = output[0][\"score\"]\n",
        "    positive_score = output[2][\"score\"]\n",
        "    assignment = np.argmax([negative_score, positive_score])\n",
        "    y_pred.append(assignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"Create and print the classification report\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fum3MTSyymlW"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3WT4jzoNZE"
      },
      "source": [
        "## **Classification Tasks that Leverage Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yuSP3heMzT"
      },
      "source": [
        "### Supervised Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGV9VS4bhq7f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert text to embeddings\n",
        "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True)\n",
        "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5L5CLcOxIeA"
      },
      "outputs": [],
      "source": [
        "train_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A7oTxoph6bn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a Logistic Regression on our train embeddings\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(train_embeddings, data[\"train\"][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFvO9KhMokF7"
      },
      "outputs": [],
      "source": [
        "# Predict previously unseen instances\n",
        "y_pred = clf.predict(test_embeddings)\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGIHxXpJgrC"
      },
      "source": [
        "**Tip!**  \n",
        "\n",
        "What would happen if we would not use a classifier at all? Instead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f_DnG1uJ7Sk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Average the embeddings of all documents in each target label\n",
        "df = pd.DataFrame(np.hstack([train_embeddings, np.array(data[\"train\"][\"label\"]).reshape(-1, 1)]))\n",
        "averaged_target_embeddings = df.groupby(768).mean().values\n",
        "\n",
        "# Find the best matching embeddings between evaluation documents and target embeddings\n",
        "sim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCWdzjMIjzx0"
      },
      "source": [
        "### Zero-shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSj6CdAetsNp"
      },
      "outputs": [],
      "source": [
        "# Create embeddings for our labels\n",
        "label_embeddings = model.encode([\"A negative review\",  \"A positive review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEIN7XnbtsQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Find the best matching label for each document\n",
        "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LyeuEUxIbW"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox27Rg71zclg"
      },
      "source": [
        "**Tip!**  \n",
        "\n",
        "What would happen if you were to use different descriptions? Use **\"A very negative movie review\"** and **\"A very positive movie review\"** to see what happens!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CC9iEGcuUit"
      },
      "source": [
        "## **Classification with Generative Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFPPzUHoEESB"
      },
      "source": [
        "### Encoder-decoder Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVbTUMktEfJ3"
      },
      "outputs": [],
      "source": [
        "# Load our model\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-small\",\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5nWQORcFlNn"
      },
      "outputs": [],
      "source": [
        "# Prepare our data\n",
        "prompt = \"Is the following sentence positive or negative? \"\n",
        "data = data.map(lambda example: {\"t5\": prompt + example['text']})\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nas574KFFSvR"
      },
      "outputs": [],
      "source": [
        "# Run inference\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
        "    text = output[0][\"generated_text\"]\n",
        "    y_pred.append(0 if text == \"negative\" else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wk2i856GnCv"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4V9iq_EELWx"
      },
      "source": [
        "### ChatGPT for Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tES6HFOwNjF6"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Create client\n",
        "client = openai.OpenAI(api_key=\"YOUR_KEY_HERE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGiovm3wyCOz"
      },
      "outputs": [],
      "source": [
        "def chatgpt_generation(prompt, document, model=\"gpt-3.5-turbo-0125\"):\n",
        "    \"\"\"Generate an output based on a prompt and an input document.\"\"\"\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\":   prompt.replace(\"[DOCUMENT]\", document)\n",
        "            }\n",
        "    ]\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=messages,\n",
        "      model=model,\n",
        "      temperature=0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL_kMwQEvMcd"
      },
      "outputs": [],
      "source": [
        "# Define a prompt template as a base\n",
        "prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n",
        "\n",
        "[DOCUMENT]\n",
        "\n",
        "If it is positive return 1 and if it is negative return 0. Do not give any other answers.\n",
        "\"\"\"\n",
        "\n",
        "# Predict the target using GPT\n",
        "document = \"unpretentious , charming , quirky , original\"\n",
        "chatgpt_generation(prompt, document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea-8XYpY3jp6"
      },
      "source": [
        "The next step would be to run one of OpenAI's model against the entire evaluation dataset. However, only run this when you have sufficient tokens as this will call the API for the entire test dataset (1066 records)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEyGElIv25Aq"
      },
      "outputs": [],
      "source": [
        "# You can skip this if you want to save your (free) credits\n",
        "predictions = [chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B07O8wtZ05x1"
      },
      "outputs": [],
      "source": [
        "# Extract predictions\n",
        "y_pred = [int(pred) for pred in predictions]\n",
        "\n",
        "# Evaluate performance\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}