{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/Unsloth_finetuning/unsloth_llm_traininig_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dAymHk6Dnvfk"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRXiHUuzlvyD"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "# Replace transformers imports for model/tokenizer with Unsloth's FastLanguageModel\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import os\n",
        "\n",
        "# --- Configuration for Unsloth LoRA ---\n",
        "# Define the rank and target modules for LoRA\n",
        "# These are common recommended settings for Unsloth\n",
        "MAX_SEQ_LENGTH = 1024 # Will be overwritten by command-line arg\n",
        "DTYPE = torch.bfloat16 # None for auto detection. torch.bfloat16 recommended for newer GPUs.\n",
        "LOAD_IN_4BIT = True # Use 4bit quantization (QLoRA) to reduce memory\n",
        "R = 16 # LoRA rank\n",
        "LORA_ALPHA = 16 # LoRA scaling factor\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "    \"embed_tokens\",  # Added for better performance on some models\n",
        "    \"lm_head\",       # Added for better performance on some models\n",
        "]\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples, tokenizer, max_length):\n",
        "    \"\"\"Tokenizes and formats data for Causal Language Modeling in a standard instruction format.\"\"\"\n",
        "    # Use a simple instruction format\n",
        "    text = [f\"Input: {q.strip()}\\nOutput: {a.strip()}{tokenizer.eos_token}\" for q, a in zip(examples[\"Input\"], examples[\"output\"])]\n",
        "\n",
        "    # Tokenizer is now the Unsloth tokenizer, but standard transformers function works\n",
        "    model_inputs = tokenizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\", # Pad to max_length for Trainer compatibility\n",
        "    )\n",
        "    # Causal Language Modeling requires labels to be the input IDs\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "def main(data_path, model_name, output_dir, final_model_dir, learning_rate, train_batch_size, eval_batch_size, num_train_epochs, max_seq_length, device_index):\n",
        "    \"\"\"\n",
        "    Loads data, sets up the model and tokenizer using Unsloth's FastLanguageModel,\n",
        "    tokenizes the dataset, runs training, and saves the LoRA adapters.\n",
        "    \"\"\"\n",
        "    print(\"Starting Unsloth LoRA fine-tuning script...\")\n",
        "\n",
        "    # 1. Load and prepare data\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    try:\n",
        "        # FIX: Handle potential ParserError with malformed CSV by using 'python' engine and skipping bad lines\n",
        "        data = pd.read_csv(data_path, engine='python', on_bad_lines='warn')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Data file not found at {data_path}\")\n",
        "        return\n",
        "\n",
        "    data = data.dropna()\n",
        "    print(f\"Loaded {len(data)} rows after dropping nulls.\")\n",
        "\n",
        "    # Convert pandas dataframe to Hugging Face DatasetDict\n",
        "    train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "    dataset = DatasetDict({\n",
        "        'train': train_dataset,\n",
        "        'test': test_dataset\n",
        "    })\n",
        "    print(f\"Dataset split: Train={len(train_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # 2. Load Model and Tokenizer using Unsloth's FastLanguageModel\n",
        "    print(f\"Loading Unsloth model and tokenizer: {model_name}...\")\n",
        "\n",
        "    # --- Unsloth Model Loading (replaces AutoTokenizer and AutoModelForCausalLM) ---\n",
        "    # Unsloth handles the device selection and quantization internally.\n",
        "    # The device_index argument is implicitly handled by Unsloth/PyTorch's environment setup\n",
        "    # or can be set via CUDA_VISIBLE_DEVICES outside of the script.\n",
        "    # For simplicity, we remove the explicit torch.device logic.\n",
        "    try:\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = model_name,\n",
        "            max_seq_length = max_seq_length,\n",
        "            dtype = DTYPE,\n",
        "            load_in_4bit = LOAD_IN_4BIT, # Enables QLoRA\n",
        "            # If using a gated model, uncomment and provide your Hugging Face token:\n",
        "            # token = \"hf_...\",\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Unsloth model or tokenizer: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. Apply LoRA adapters\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = R,\n",
        "        target_modules = TARGET_MODULES,\n",
        "        lora_alpha = LORA_ALPHA,\n",
        "        lora_dropout = 0, # Unsloth optimized: set to 0\n",
        "        bias = \"none\",    # Unsloth optimized: set to \"none\"\n",
        "        use_gradient_checkpointing = \"unsloth\", # Optimized gradient checkpointing\n",
        "        random_state = 3407,\n",
        "        use_rslora = False,\n",
        "    )\n",
        "\n",
        "    print(f\"LoRA adapters applied. Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    # 4. Tokenize Dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    def wrapped_preprocess(examples):\n",
        "        return preprocess_function(examples, tokenizer, max_length=max_seq_length)\n",
        "\n",
        "    tokenized_datasets = dataset.map(\n",
        "        wrapped_preprocess,\n",
        "        batched=True,\n",
        "        remove_columns=['Input', 'output', '__index_level_0__']\n",
        "    )\n",
        "\n",
        "    # 5. Setup Training Arguments and Trainer (TRL's SFTTrainer is often used with Unsloth,\n",
        "    # but the standard Hugging Face Trainer is also fully compatible and used here for simplicity)\n",
        "    print(\"Setting up Trainer...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        per_device_eval_batch_size=eval_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        # Add recommended arguments for LoRA/QLoRA training\n",
        "        fp16=not torch.cuda.is_available() or DTYPE != torch.bfloat16, # Use FP16 if bfloat16 is not available/used\n",
        "        bf16=torch.cuda.is_available() and DTYPE == torch.bfloat16,   # Use BF16 if available and specified\n",
        "        gradient_accumulation_steps=1, # Adjust if batch size is too small\n",
        "        # Log Unsloth's performance\n",
        "        report_to=\"none\", # Change to \"wandb\" or \"tensorboard\" if needed\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "    )\n",
        "\n",
        "    # 6. Start Training\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    # 7. Save the final LoRA adapters and tokenizer\n",
        "    # Unsloth saves only the small LoRA adapters by default with save_pretrained\n",
        "    print(f\"Saving final LoRA adapters and tokenizer to {final_model_dir}...\")\n",
        "    os.makedirs(final_model_dir, exist_ok=True)\n",
        "    # Saves the LoRA adapters\n",
        "    trainer.model.save_pretrained(final_model_dir)\n",
        "    # Saves the Unsloth tokenizer\n",
        "    tokenizer.save_pretrained(final_model_dir)\n",
        "\n",
        "    # OPTIONAL: Save the merged model (base model + LoRA weights)\n",
        "    # This is needed to use the model without Unsloth or Peft/LoRA\n",
        "    print(\"\\n--- OPTIONAL: Merging and saving full model ---\")\n",
        "    merged_model_dir = final_model_dir + \"_merged\"\n",
        "    print(f\"Merging LoRA adapters into base model and saving to {merged_model_dir}...\")\n",
        "    model.save_pretrained_merged(merged_model_dir, tokenizer, save_method=\"merged_16bit\",)\n",
        "    print(\"Full merged model saved successfully.\")\n",
        "\n",
        "    print(\"Model and tokenizer saved successfully.\")\n",
        "\n",
        "# --- Command Line Argument Parser ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune a Causal Language Model on a custom dataset using Unsloth LoRA.\")\n",
        "\n",
        "    # Required and Model Arguments (same as original)\n",
        "    parser.add_argument(\"--data_path\", type=str, default=\"/content/all_medtext.csv\", required=True, help=\"Path to the input CSV data file.\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default=\"unsloth/Llama-3.2-1B\", help=\"Hugging Face model ID.\")\n",
        "\n",
        "    # Training Arguments (same as original)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./results\", help=\"Directory for Trainer checkpoints and logs.\")\n",
        "    parser.add_argument(\"--final_model_dir\", type=str, default=\"./finetuned_model_unsloth\", help=\"Directory to save the final fine-tuned LoRA adapters and tokenizer.\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5, help=\"Learning rate for the optimizer.\")\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=8, help=\"Batch size per device for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=8, help=\"Batch size per device for evaluation.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=3, help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_seq_length\", type=int, default=1024, help=\"Maximum sequence length for tokenization.\")\n",
        "\n",
        "    # GPU Node Selection (kept for command-line compatibility, but Unsloth handles it internally)\n",
        "    parser.add_argument(\n",
        "        \"--device_index\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"Index of the GPU device to use for training (Unsloth typically uses the best available GPU or 0 by default).\"\n",
        "    )\n",
        "\n",
        "    # Instead of calling parse_args(), which expects command-line arguments\n",
        "    # and fails if a required argument is not provided, we will manually\n",
        "    # provide the default values to the main function. This allows the script\n",
        "    # to run directly in a Colab cell without needing explicit command-line args.\n",
        "\n",
        "    main(\n",
        "        data_path=\"/content/all_medtext.csv\",\n",
        "        model_name=\"unsloth/Llama-3.2-1B\",\n",
        "        output_dir=\"./results\",\n",
        "        final_model_dir=\"./finetuned_model_unsloth\",\n",
        "        learning_rate=2e-1,\n",
        "        train_batch_size=64,\n",
        "        eval_batch_size=64,\n",
        "        num_train_epochs=3,\n",
        "        max_seq_length=1024,\n",
        "        device_index=0 # Unsloth typically handles this automatically\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xPRV_8Xsae1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyMFD907aiLS+ZQkfVM31iHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}